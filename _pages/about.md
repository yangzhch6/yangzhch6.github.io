---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


About me
===
<!-- I obtained my Master's degree at the [HCP Lab](http://www.sysu-hcp.net/home/) and bachelor's degree in SYSU, where I was fortunately advised by [Prof. Xiaodan Liang](https://scholar.google.com/citations?user=voxznZAAAAAJ&hl=zh-CN) to conduct research in NLP. -->


<!-- Research Interests
=== -->
Bro is currently pursuing a PhD degree with a keen focus on Reasoning with Large Language Models (LLMs). 
Specifically, I focus on the followings:
* Solve & Verify
* Reasoning Data Synthesis
* Test-Time Scaling


> If youâ€™re interested in collaborating or exploring potential research opportunities, please donâ€™t hesitate to reach out (å¸¦å¸¦å“¥ä»¬). 


News
===
- **<font style = "color:#FF8000">[9/2025]</font>** Two papers accepted to ***EMNLP 2025*** Findings.
- **<font style = "color:#FF8000">[1/2025]</font>** We are organizing the [2nd AI4MATH workshop @ ICML 2025](https://sites.google.com/view/ai4mathworkshopicml2025).
- **<font style = "color:#FF8000">[1/2025]</font>** One paper accepted to ***ICLR 2025***.
- **<font style = "color:#FF8000">[9/2024]</font>** One paper accepted to ***Findings of EMNLP 2024***.
- **<font style = "color:#FF8000">[5/2024]</font>** *Serve as the challenge lead organizer of [Automated Optimization Problem-Solving with Code](https://www.codabench.org/competitions/2438/) in [AI for Math Workshop and Challenges](https://sites.google.com/view/ai4mathworkshopicml2024) at [ICML 2024](https://icml.cc/Conferences/2024).*


Preprints
===
<strong><font style = "color:#1f57b8">ðŸ”¥Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</font></strong><br />
<strong>Zhicheng Yang</strong>, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang<br />
[[Paper]](https://arxiv.org/pdf/2508.13755) [[Code]](https://github.com/yangzhch6/DARS) <br /> 

<strong><font style = "color:#1f57b8">TreeRPO: Tree Relative Policy Optimization</font></strong><br />
<strong>Zhicheng Yang</strong>, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang<br />
[[Paper]](https://arxiv.org/pdf/2506.05183) [[Code]](https://github.com/yangzhch6/TreeRPO) <br /> 

<strong><font style = "color:#1f57b8">Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers</font></strong><br />
<strong>Zhicheng Yang</strong>, Zhijiang Guo, Yinya Huang, Yongxin Wang, Yiwei Wang, Xiaodan Liang, Jing Tang<br />
[[Paper]](https://arxiv.org/abs/2509.23152) [[Code]](https://github.com/yangzhch6/Mirror-Critique) <br /> 


Selected Publication 
===
<strong><font style = "color:#1f57b8">OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling</font></strong><br />
<strong>Zhicheng Yang</strong>, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi Song, Xiaodan Liang, Jing Tang<br />
The Thirteenth International Conference on Learning Representations (ICLR 2025) <br />
[[Paper]](https://arxiv.org/abs/2407.09887) [[Code]](https://github.com/yangzhch6/ReSocratic) <br /> 

<strong><font style = "color:#1f57b8">AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations</font></strong><br />
<strong>Zhicheng Yang</strong>, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang <br />
The 2024 Conference on Empirical Methods in Natural Language Processing. (Findings of EMNLP 2024) <br />
[[Paper]](https://aclanthology.org/2024.findings-emnlp.163/) [[Code]](https://github.com/yangzhch6/AlignedCoT) <br /> 

<strong><font style = "color:#1f57b8">LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning</font></strong><br />
<strong>Zhicheng Yang<sup>*</sup></strong>, Jinghui Qin<sup>*</sup>, Jiaqi Chen, Liang Lin, Xiaodan Liang<br />
The 2022 Conference on Empirical Methods in Natural Language Processing. (Findings of EMNLP 2022) <br />
[[Paper]](https://anthology.aclweb.org/2022.findings-emnlp.1/) [[Code]](https://github.com/yangzhch6/InterMWP)<br />

<strong><font style = "color:#1f57b8">Unbiased Math Word Problems Benchmark for Mitigating Solving Bias</font></strong><br />
<strong>Zhicheng Yang</strong>, Jinghui Qin, Jiaqi Chen, Xiaodan Liang<br />
Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2022. (Findings of NAACL 2022)<br />
[[Paper]](https://aclanthology.org/2022.findings-naacl.104/) [[Code]](https://github.com/yangzhch6/UnbiasedMWP) <br />

<strong><font style = "color:#1f57b8">CLOMO: Counterfactual Logical Modification with Large Language Models</font></strong><br />
Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, <strong>Zhicheng Yang</strong>, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song <br />
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. (ACL 2024) <br />
[[Paper]](https://arxiv.org/abs/2311.17438) <br />

<strong><font style = "color:#1f57b8">DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</font></strong><br />
Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, <strong>Zhicheng Yang</strong>, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang <br />
12th International Conference on Learning Representations, 2024. (ICLR 2024)<br />
[[Paper]](https://arxiv.org/abs/2310.02954) <br />

<strong><font style = "color:#1f57b8">ATG: Benchmarking Automated Theorem Generation for Generative Language Models</font></strong><br />
Xiaohan Lin, Qingxing Cao, Yinya Huang, **Zhicheng Yang**, Zhengying Liu, Zhenguo Li, Xiaodan Liang <br />
Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2024. (Findings of NAACL 2024)<br />
[[Paper]](https://openreview.net/forum?id=H0RzzhAxTv&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FNAACL%2F2024%2FConference%2FAuthors%23your-submissions)) <br />

<strong><font style = "color:#1f57b8">Template-based Contrastive Distillation Pre-training for Math Word Problem Solving</font></strong><br />
Jinghui Qin*, <strong>Zhicheng Yang*</strong>, Jiaqi Chen, Xiaodan Liang and Liang Lin<br />
IEEE Transactions on Neural Networks and Learning Systems, 2023. (TNNLS) <br />
[[Paper]](https://ieeexplore.ieee.org/document/10113691) <br />

(* denotes co-first authors) <br />


Education
===
* 2020 --- 2023: **Master** in Pattern Recognition and Intelligent Systems, Sun Yat-sen University (Shenzhen)
* 2016 --- 2020: **B.Sc.** in Computer Science and Technology, Sun Yat-sen University (Panyu)


Honors and Awards
===
* National First Prize, Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM), China
* First Prize Scholarship, Sun Yat-sen University

Experience
===
* <div>LLM Research Intern, ByteDance-Seed</div> 
* <div>NLP Research Intern, Huawei Noah's Ark</div> 
* <div>Recommender System Intern, ByteDance-Data-Douyin</div> 
* <div>NLP Research Intern, DMAI</div> 

---
<script>
document.write("Last modifid at: "+document.lastModified+"" )
</script>

<a href="https://info.flagcounter.com/kdvh"><img src="https://s11.flagcounter.com/map/kdvh/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>
