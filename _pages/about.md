---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Short Bio
===
I'm **Zhicheng YANG**, who is pursuing a Ph.D. degree. I obtained my Master's degree at the [HCP Lab](http://www.sysu-hcp.net/home/) and bachelor's degree of Computer Science and Technology in SYSU, where I was fortunately advised by [Prof. Xiaodan Liang](https://scholar.google.com/citations?user=voxznZAAAAAJ&hl=zh-CN) to conduct research in NLP.


Research Interests
===
I have a broad interest in utilizing Large Models (LMs) for **complex reasoning** tasks.

With the training text that would take 10^4 years for a human to read, LLMs excel beyond human capability in the domains of writing, translation, and knowledge reservoirs.
However, their mathematical ability is still at the level of primary and secondary school students. 
To fill this confusing and unignorable gap, I'm interested in the following topics:
* Automatic Data Synthesis
* Self-Evolution Framework
* Learning Efficiency (performance improvement / data quantity)


News
===
- One paper accepted to ***Findings of EMNLP 2024***.
- **<font style = "color:#FF8000">[5/2024]</font>** *Serve as the challenge lead organizer of [Automated Optimization Problem-Solving with Code](https://www.codabench.org/competitions/2438/) in [AI for Math Workshop and Challenges](https://sites.google.com/view/ai4mathworkshopicml2024) at [ICML 2024](https://icml.cc/Conferences/2024).*


Preprints
===
<strong><font style = "color:#1f57b8">Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis</font></strong><br />
<strong>Zhicheng Yang</strong>, Yinya Huang, Wei Shi, Liang Feng, Linqi Song, Yiwei Wang, Xiaodan Liang, Jing Tang <br />
[[Paper]](https://arxiv.org/abs/2407.09887) [[Code]](https://github.com/yangzhch6/ReSocratic) <br /> 


Recent Publication 
===
<strong><font style = "color:#1f57b8">AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations</font></strong><br />
<strong>Zhicheng Yang</strong>, Yinya Huang, Jing Xiong, Liang Feng, Xiaodan Liang, Yiwei Wang, Jing Tang <br />
The 2024 Conference on Empirical Methods in Natural Language Processing. (Findings of EMNLP 2024) <br />
[[Paper]](https://arxiv.org/abs/2311.13538) [[Code]](https://github.com/yangzhch6/AlignedCoT) <br /> 

<strong><font style = "color:#1f57b8">LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning</font></strong><br />
<strong>Zhicheng Yang<sup>*</sup></strong>, Jinghui Qin<sup>*</sup>, Jiaqi Chen, Liang Lin, Xiaodan Liang<br />
The 2022 Conference on Empirical Methods in Natural Language Processing. (Findings of EMNLP 2022) <br />
[[Paper]](https://anthology.aclweb.org/2022.findings-emnlp.1/) [[Code]](https://github.com/yangzhch6/InterMWP)<br />

<strong><font style = "color:#1f57b8">Unbiased Math Word Problems Benchmark for Mitigating Solving Bias</font></strong><br />
<strong>Zhicheng Yang</strong>, Jinghui Qin, Jiaqi Chen, Xiaodan Liang<br />
Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2022. (Findings of NAACL 2022)<br />
[[Paper]](https://aclanthology.org/2022.findings-naacl.104/) [[Code]](https://github.com/yangzhch6/UnbiasedMWP) <br />

<strong><font style = "color:#1f57b8">CLOMO: Counterfactual Logical Modification with Large Language Models</font></strong><br />
Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, <strong>Zhicheng Yang</strong>, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song <br />
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. (ACL 2024) <br />
[[Paper]](https://arxiv.org/abs/2311.17438) <br />

<strong><font style = "color:#1f57b8">DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</font></strong><br />
Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, <strong>Zhicheng Yang</strong>, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang <br />
12th International Conference on Learning Representations, 2024. (ICLR 2024)<br />
[[Paper]](https://arxiv.org/abs/2310.02954) <br />

<strong><font style = "color:#1f57b8">ATG: Benchmarking Automated Theorem Generation for Generative Language Models</font></strong><br />
Xiaohan Lin, Qingxing Cao, Yinya Huang, **Zhicheng Yang**, Zhengying Liu, Zhenguo Li, Xiaodan Liang <br />
Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2024. (Findings of NAACL 2024)<br />
[[Paper]](https://openreview.net/forum?id=H0RzzhAxTv&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FNAACL%2F2024%2FConference%2FAuthors%23your-submissions)) <br />

<strong><font style = "color:#1f57b8">Template-based Contrastive Distillation Pre-training for Math Word Problem Solving</font></strong><br />
Jinghui Qin*, <strong>Zhicheng Yang*</strong>, Jiaqi Chen, Xiaodan Liang and Liang Lin<br />
IEEE Transactions on Neural Networks and Learning Systems, 2023. (TNNLS) <br />
[[Paper]](https://ieeexplore.ieee.org/document/10113691) <br />

(* denotes co-first authors) <br />


Education
===
* 2020 --- 2023: **Mphil** in Pattern Recognition and Intelligent Systems, School of Intelligent Systems Engineering, Sun Yat-sen University. 
* 2016 --- 2020: **B.Sc.** in Computer Science and Engineering, School of Computer Science and Engineering, Sun Yat-sen University.


Honors and Awards
===
* National First Prize, Contemporary Undergraduate Mathematical Contest in Modeling (CUMCM), China
* First Prize Scholarship, Sun Yat-sen University
* Second Prize Scholarship, Sun Yat-sen University

Experience
===
* <div>NLP Research Intern, Huawei Noah's Ark</div> 
* <div>Recommender System Intern, ByteDance-Data-Douyin</div> 
* <div>NLP Research Intern, DMAI</div> 

---
<script>
document.write("Last modifid at: "+document.lastModified+"" )
</script>

<a href="https://info.flagcounter.com/kdvh"><img src="https://s11.flagcounter.com/map/kdvh/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>
